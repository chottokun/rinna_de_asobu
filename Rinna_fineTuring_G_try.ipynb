{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rinna_fineTuring_G_try.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7UV0g2EX9S60qwsgwkhc+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2でファインチューニングしたモデルから文章生成する。\n",
        "\n",
        "公開されているRinnaの学習モデルをファインチューニングしたモデルから文章の生成を行う。\n",
        "\n",
        "## 環境\n",
        "\n",
        " - Clab PRO\n",
        " - Google Drive\n",
        "\n",
        "## ディレクトリ\n",
        "\n",
        " - ./work : transformersのcode（一部改変して使うのでGoogle Driveで永続化)\n",
        " - ./output : 学習したモデルを保存\n",
        "\n",
        "## 学習ファイルの形式\n",
        "\n",
        "人物：セリフ\n",
        "\n",
        "例「アムロ：シャア、なぜわからないんだ」\n",
        "\n",
        "## 参考にしたサイト\n",
        "\n",
        " - https://note.com/npaka/n/n8a435f0c8f69\n"
      ],
      "metadata": {
        "id": "uwaE-pyPfaxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの永続化(Google Driveへの接続)\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p '/content/drive/My Drive/work/'\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApCCI8b1fjX_",
        "outputId": "055b35b3-23fa-4a80-d5df-ba0fd242f9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface Transformers\n",
        "# e オプションができなくなってた・・・。\n",
        "!git clone https://github.com/huggingface/transformers -b v4.4.2\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9gfK5Xcfngy",
        "outputId": "ad8a4493-d1b9-4d28-d197-17366b9322ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 作業フォルダへ\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfIgjpqWfrkm",
        "outputId": "b12071fd-eda2-48e3-b604-90a0f0232d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface Datasets\n",
        "!pip install datasets==1.2.1\n",
        "# Sentencepiece\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B8qH_7-fs9V",
        "outputId": "910d8524-6529-4440-bad6-2173cba85fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (6.0.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.49.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (0.1.91)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizerの変更\n",
        "\n",
        "「rinna」の日本語GPT-2モデルは「AutoTokenizer」ではなく「T5Tokenizer」を使っている。\n",
        "ファインチューニングに用いる「./transformers/examples/language-modeling/run_clm.py」を予め編集しておく。\n",
        "\n",
        "トレーニング用セリフ集は「train_g01.txt」とした。"
      ],
      "metadata": {
        "id": "QUOeQKizf4a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "# ./output　があっても上書きする　--overwrite_output\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train_g01.txt \\\n",
        "    --validation_file=train_g01.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=30 \\\n",
        "    --save_steps=10000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/ \\\n",
        "    --overwrite_output_dir \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy1IrQOgf57I",
        "outputId": "a38d8eb8-d736-4b75-d6b4-e5f07c91d464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/04/2022 01:03:39 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "05/04/2022 01:03:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/May04_01-03-39_84e233aea4f7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=10000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-860ff375ed309f5f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:654] 2022-05-04 01:03:40,326 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f57ebfd37e088140858c25efd76026dba0044e364b7afd55f2cf592598e37324.d7f8d739b24053db52ad34244d36a3df6a930153af5d4a1445ac61cac8bf8bc3\n",
            "[INFO|configuration_utils.py:690] 2022-05-04 01:03:40,328 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"rinna/japanese-gpt2-small\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": 3072,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-04 01:03:41,460 >> loading file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/d22119c30bc106b290ef639d61c14ee4df55beda7ad6b2e258932e5a270908a2.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-04 01:03:41,460 >> loading file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-04 01:03:41,460 >> loading file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/96f1ddf7247675414b491c5c3ae4c7ac307cf28d0f90977b7cbdf13a8f84bfc8.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-04 01:03:41,460 >> loading file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f58aaff203818bdc42aa91ed2a00a99b97cd62db4b09c36e93903f6e95fba3bc.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|modeling_utils.py:1772] 2022-05-04 01:03:41,818 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/08dce51a6284b7dfd687564d1c972f9a499e246d34d08a6344a271d6913cf3f5.1f6f511ba62456bd0ab11dc83c05711adfb07d790dbf93d77ad761e1b1c03e19\n",
            "[INFO|modeling_utils.py:2057] 2022-05-04 01:03:43,513 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2066] 2022-05-04 01:03:43,513 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:195: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-860ff375ed309f5f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-4ab5f52f9a3366c4.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-860ff375ed309f5f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-b3300d3a08a04716.arrow\n",
            "./transformers/examples/language-modeling/run_clm.py:338: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
            "05/04/2022 01:03:43 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-860ff375ed309f5f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-7a189291cec087c0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-860ff375ed309f5f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-47f7092f3c766337.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-05-04 01:03:43,602 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-05-04 01:03:43,602 >>   Num examples = 23\n",
            "[INFO|trainer.py:1292] 2022-05-04 01:03:43,602 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1293] 2022-05-04 01:03:43,602 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1294] 2022-05-04 01:03:43,603 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1295] 2022-05-04 01:03:43,603 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-05-04 01:03:43,603 >>   Total optimization steps = 690\n",
            "{'loss': 1.5215, 'learning_rate': 1.3768115942028985e-05, 'epoch': 21.74}\n",
            "100% 690/690 [3:34:52<00:00, 18.74s/it][INFO|trainer.py:1530] 2022-05-04 04:38:36,354 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 12892.7512, 'train_samples_per_second': 0.054, 'train_steps_per_second': 0.054, 'train_loss': 1.264979951278023, 'epoch': 30.0}\n",
            "100% 690/690 [3:34:52<00:00, 18.69s/it]\n",
            "[INFO|trainer.py:2166] 2022-05-04 04:38:36,359 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:441] 2022-05-04 04:38:36,717 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:1378] 2022-05-04 04:38:38,537 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-05-04 04:38:38,888 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-05-04 04:38:39,537 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =      1.265\n",
            "  train_runtime            = 3:34:52.75\n",
            "  train_samples            =         23\n",
            "  train_samples_per_second =      0.054\n",
            "  train_steps_per_second   =      0.054\n",
            "05/04/2022 04:38:41 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:2416] 2022-05-04 04:38:41,328 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-05-04 04:38:41,328 >>   Num examples = 23\n",
            "[INFO|trainer.py:2421] 2022-05-04 04:38:41,328 >>   Batch size = 1\n",
            "100% 23/23 [02:09<00:00,  5.62s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_loss               =     0.3552\n",
            "  eval_runtime            = 0:02:15.02\n",
            "  eval_samples            =         23\n",
            "  eval_samples_per_second =       0.17\n",
            "  eval_steps_per_second   =       0.17\n",
            "  perplexity              =     1.4265\n",
            "CPU times: user 1min 41s, sys: 15 s, total: 1min 56s\n",
            "Wall time: 3h 37min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error で動かないので対策。"
      ],
      "metadata": {
        "id": "akA-bfycIpps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install -U numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "XyeqUNf4HvGi",
        "outputId": "90de4562-0b20-43c3-c450-49df1ee6feca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.21.6\n",
            "Uninstalling numpy-1.21.6:\n",
            "  Successfully uninstalled numpy-1.21.6\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 345 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# テキスト生成のテスト"
      ],
      "metadata": {
        "id": "td6Fl6JMIy2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# できたモデルから生成\n",
        "input = tokenizer.encode(r\"シャア:ララァは母になるべき女だったんだ\", return_tensors=\"pt\")\n",
        "output = model.generate(input,\n",
        "                        do_sample=True,\n",
        "                        top_p=0.90,\n",
        "                        top_k=0,\n",
        "                        max_length=100,\n",
        "                        skip_special_tokens=True,\n",
        "                        pad_token_id=tokenizer.pad_token_id,\n",
        "                        bos_token_id=tokenizer.bos_token_id,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        bad_word_ids=[[tokenizer.unk_token_id]]\n",
        "                        )\n",
        "\n",
        "# セリフ用の整形はせずにとりあえず、出力。\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_xr0CsLz5YE",
        "outputId": "6e4b3d16-60b0-421c-b128-c589da985169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "シャア:ララァは母になるべき女だったんだ アムロ そうか\n"
          ]
        }
      ]
    }
  ]
}